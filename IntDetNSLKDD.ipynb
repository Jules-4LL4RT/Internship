{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IntDetNSLKDD.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "17DHzFeR5vjDEdkEAWaPGg5iJkTwcXvaz",
      "authorship_tag": "ABX9TyPGB0mm92wpMp2zjNpTTmRv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jules-4LL4RT/Internship/blob/main/IntDetNSLKDD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ESukQBA4Eji"
      },
      "outputs": [],
      "source": [
        "!pip3 install pyspark\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
        "from sklearn import preprocessing\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "import math\n",
        "import itertools\n",
        "import pandas\n",
        "import seaborn as sns\n",
        "from time import time\n",
        "from collections import OrderedDict\n",
        "%matplotlib inline\n",
        "gt0 = time()\n",
        "from pyspark.sql import SQLContext, Row\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import udf, split, col\n",
        "import pyspark.sql.functions as sql\n",
        "from pyspark.ml import Pipeline, Transformer\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark import keyword_only\n",
        "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param\n",
        "import sklearn.metrics as metrics\n",
        "from pyspark.ml.feature import VectorSlicer\n",
        "from pyspark.ml.feature import PCA\n",
        "import pyspark\n",
        "\n",
        "sc = pyspark.SparkContext(master='local[8]')\n",
        "sc.setLogLevel('INFO')\n",
        "sqlContext = SQLContext(sc)\n",
        "\n",
        "\n",
        "train20_nsl_kdd_dataset_path = \"/content/KDDTrain_20Percent.txt\"\n",
        "train_nsl_kdd_dataset_path = \"/content/KDDTrain.txt\"\n",
        "test_nsl_kdd_dataset_path = \"/content/KDDTest.txt\"\n",
        "\n",
        "col_names = np.array([\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n",
        "    \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
        "    \"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
        "    \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
        "    \"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
        "    \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
        "    \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
        "    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
        "    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
        "    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"labels\"])\n",
        "\n",
        "nominal_inx = [1, 2, 3]\n",
        "binary_inx = [6, 11, 13, 14, 20, 21]\n",
        "numeric_inx = list(set(range(41)).difference(nominal_inx).difference(binary_inx))\n",
        "\n",
        "nominal_cols = col_names[nominal_inx].tolist()\n",
        "binary_cols = col_names[binary_inx].tolist()\n",
        "numeric_cols = col_names[numeric_inx].tolist()\n",
        "\n",
        "def load_dataset(path):\n",
        "    dataset_rdd = sc.textFile(path, 8).map(lambda line: line.split(','))\n",
        "    dataset_df = (dataset_rdd.toDF(col_names.tolist()).select(\n",
        "                    col('duration').cast(DoubleType()),\n",
        "                    col('protocol_type').cast(StringType()),\n",
        "                    col('service').cast(StringType()),\n",
        "                    col('flag').cast(StringType()),\n",
        "                    col('src_bytes').cast(DoubleType()),\n",
        "                    col('dst_bytes').cast(DoubleType()),\n",
        "                    col('land').cast(DoubleType()),\n",
        "                    col('wrong_fragment').cast(DoubleType()),\n",
        "                    col('urgent').cast(DoubleType()),\n",
        "                    col('hot').cast(DoubleType()),\n",
        "                    col('num_failed_logins').cast(DoubleType()),\n",
        "                    col('logged_in').cast(DoubleType()),\n",
        "                    col('num_compromised').cast(DoubleType()),\n",
        "                    col('root_shell').cast(DoubleType()),\n",
        "                    col('su_attempted').cast(DoubleType()),\n",
        "                    col('num_root').cast(DoubleType()),\n",
        "                    col('num_file_creations').cast(DoubleType()),\n",
        "                    col('num_shells').cast(DoubleType()),\n",
        "                    col('num_access_files').cast(DoubleType()),\n",
        "                    col('num_outbound_cmds').cast(DoubleType()),\n",
        "                    col('is_host_login').cast(DoubleType()),\n",
        "                    col('is_guest_login').cast(DoubleType()),\n",
        "                    col('count').cast(DoubleType()),\n",
        "                    col('srv_count').cast(DoubleType()),\n",
        "                    col('serror_rate').cast(DoubleType()),\n",
        "                    col('srv_serror_rate').cast(DoubleType()),\n",
        "                    col('rerror_rate').cast(DoubleType()),\n",
        "                    col('srv_rerror_rate').cast(DoubleType()),\n",
        "                    col('same_srv_rate').cast(DoubleType()),\n",
        "                    col('diff_srv_rate').cast(DoubleType()),\n",
        "                    col('srv_diff_host_rate').cast(DoubleType()),\n",
        "                    col('dst_host_count').cast(DoubleType()),\n",
        "                    col('dst_host_srv_count').cast(DoubleType()),\n",
        "                    col('dst_host_same_srv_rate').cast(DoubleType()),\n",
        "                    col('dst_host_diff_srv_rate').cast(DoubleType()),\n",
        "                    col('dst_host_same_src_port_rate').cast(DoubleType()),\n",
        "                    col('dst_host_srv_diff_host_rate').cast(DoubleType()),\n",
        "                    col('dst_host_serror_rate').cast(DoubleType()),\n",
        "                    col('dst_host_srv_serror_rate').cast(DoubleType()),\n",
        "                    col('dst_host_rerror_rate').cast(DoubleType()),\n",
        "                    col('dst_host_srv_rerror_rate').cast(DoubleType()),\n",
        "                    col('labels').cast(StringType())))\n",
        "\n",
        "    return dataset_df\n",
        "\n",
        "attack_dict = {\n",
        "    'normal': 'normal',\n",
        "    \n",
        "    'back': 'DoS',\n",
        "    'land': 'DoS',\n",
        "    'neptune': 'DoS',\n",
        "    'pod': 'DoS',\n",
        "    'smurf': 'DoS',\n",
        "    'teardrop': 'DoS',\n",
        "    'mailbomb': 'DoS',\n",
        "    'apache2': 'DoS',\n",
        "    'processtable': 'DoS',\n",
        "    'udpstorm': 'DoS',\n",
        "    \n",
        "    'ipsweep': 'Probe',\n",
        "    'nmap': 'Probe',\n",
        "    'portsweep': 'Probe',\n",
        "    'satan': 'Probe',\n",
        "    'mscan': 'Probe',\n",
        "    'saint': 'Probe',\n",
        "\n",
        "    'ftp_write': 'R2L',\n",
        "    'guess_passwd': 'R2L',\n",
        "    'imap': 'R2L',\n",
        "    'multihop': 'R2L',\n",
        "    'phf': 'R2L',\n",
        "    'spy': 'R2L',\n",
        "    'warezclient': 'R2L',\n",
        "    'warezmaster': 'R2L',\n",
        "    'sendmail': 'R2L',\n",
        "    'named': 'R2L',\n",
        "    'snmpgetattack': 'R2L',\n",
        "    'snmpguess': 'R2L',\n",
        "    'xlock': 'R2L',\n",
        "    'xsnoop': 'R2L',\n",
        "    'worm': 'R2L',\n",
        "    \n",
        "    'buffer_overflow': 'U2R',\n",
        "    'loadmodule': 'U2R',\n",
        "    'perl': 'U2R',\n",
        "    'rootkit': 'U2R',\n",
        "    'httptunnel': 'U2R',\n",
        "    'ps': 'U2R',    \n",
        "    'sqlattack': 'U2R',\n",
        "    'xterm': 'U2R'\n",
        "}\n",
        "\n",
        "attack_mapping_udf = udf(lambda v: attack_dict[v])\n",
        "\n",
        "class Labels2Converter(Transformer):\n",
        "\n",
        "    @keyword_only\n",
        "    def __init__(self):\n",
        "        super(Labels2Converter, self).__init__()\n",
        "\n",
        "    def _transform(self, dataset):\n",
        "        return dataset.withColumn('labels2', sql.regexp_replace(col('labels'), '^(?!normal).*$', 'attack'))\n",
        "     \n",
        "class Labels5Converter(Transformer):\n",
        "    \n",
        "    @keyword_only\n",
        "    def __init__(self):\n",
        "        super(Labels5Converter, self).__init__()\n",
        "\n",
        "    def _transform(self, dataset):\n",
        "        return dataset.withColumn('labels5', attack_mapping_udf(col('labels')))\n",
        "    \n",
        "labels2_indexer = StringIndexer(inputCol=\"labels2\", outputCol=\"labels2_index\")\n",
        "labels5_indexer = StringIndexer(inputCol=\"labels5\", outputCol=\"labels5_index\")\n",
        "\n",
        "labels_mapping_pipeline = Pipeline(stages=[Labels2Converter(), Labels5Converter(), labels2_indexer, labels5_indexer])\n",
        "\n",
        "labels2 = ['normal', 'attack']\n",
        "labels5 = ['normal', 'DoS', 'Probe', 'R2L', 'U2R']\n",
        "labels_col = 'labels2_index'\n",
        "\n",
        "t0 = time()\n",
        "train_df = load_dataset(train_nsl_kdd_dataset_path)\n",
        "\n",
        "# Fitting preparation pipeline\n",
        "labels_mapping_model = labels_mapping_pipeline.fit(train_df)\n",
        "\n",
        "# Transforming labels column and adding id column\n",
        "train_df = labels_mapping_model.transform(train_df).withColumn('id', sql.monotonically_increasing_id())\n",
        "\n",
        "#train_df = train_df.cache()\n",
        "#print(train_df.count())\n",
        "#print(time() - t0)\n",
        "\n",
        "#t0 = time()\n",
        "#test_df = load_dataset(test_nsl_kdd_dataset_path)\n",
        "#test_df = labels_mapping_model.transform(test_df).withColumn('id', sql.monotonically_increasing_id())\n",
        "#test_df = test_df.cache()\n",
        "#print(test_df.count())\n",
        "#print(time() - t0)\n",
        "\n",
        "#(train_df.groupby('labels2').count().show())\n",
        "#(train_df.groupby('labels5').count().sort(sql.desc('count')).show())\n",
        "\n",
        "#(test_df.groupby('labels2').count().show())\n",
        "#(test_df.groupby('labels5').count().sort(sql.desc('count')).show())\n",
        "\n",
        "#(train_df.crosstab(nominal_cols[0], 'labels2').sort(sql.asc(nominal_cols[0] + '_labels2')).show())\n",
        "#(train_df.crosstab(nominal_cols[0], 'labels5').sort(sql.asc(nominal_cols[0] + '_labels5')).show())\n",
        "\n",
        "#print(train_df.select(nominal_cols[1]).distinct().count())\n",
        "#(train_df.crosstab(nominal_cols[1], 'labels2').sort(sql.asc(nominal_cols[1] + '_labels2')).show(n=70))\n",
        "#(train_df.crosstab(nominal_cols[1], 'labels5').sort(sql.asc(nominal_cols[1] + '_labels5')).show(n=70))\n",
        "\n",
        "#(train_df.crosstab('su_attempted', 'labels2').show())\n",
        "\n",
        "#train_df = train_df.replace(2.0, 0.0, 'su_attempted')\n",
        "#test_df = test_df.replace(2.0, 0.0, 'su_attempted')\n",
        "\n",
        "#print(len(numeric_cols))\n",
        "#(train_df.select(numeric_cols).describe().toPandas().transpose())\n",
        "\n",
        "train_df = train_df.drop('num_outbound_cmds')\n",
        "test_df = test_df.drop('num_outbound_cmds')\n",
        "numeric_cols.remove('num_outbound_cmds')\n",
        "\n",
        "def ohe_vec(cat_dict, row):\n",
        "    vec = np.zeros(len(cat_dict))\n",
        "    vec[cat_dict[row]] = float(1.0)\n",
        "    return vec.tolist()\n",
        "\n",
        "def ohe(df, nominal_col):\n",
        "    categories = (df.select(nominal_col)\n",
        "                    .distinct()\n",
        "                    .rdd.map(lambda row: row[0])\n",
        "                    .collect())\n",
        "    \n",
        "    cat_dict = dict(zip(categories, range(len(categories))))\n",
        "    \n",
        "    udf_ohe_vec = udf(lambda row: ohe_vec(cat_dict, row), \n",
        "                      StructType([StructField(cat, DoubleType(), False) for cat in categories]))\n",
        "    \n",
        "    df = df.withColumn(nominal_col + '_ohe', udf_ohe_vec(col(nominal_col))).cache()\n",
        "    \n",
        "    nested_cols = [nominal_col + '_ohe.' + cat for cat in categories]\n",
        "    ohe_cols = [nominal_col + '_' + cat for cat in categories]\n",
        "        \n",
        "    for new, old in zip(ohe_cols, nested_cols):\n",
        "        df = df.withColumn(new, col(old))\n",
        "\n",
        "    df = df.drop(nominal_col + '_ohe')\n",
        "                   \n",
        "    return df, ohe_cols\n",
        "\n",
        "#t0 = time()\n",
        "#train_ohe_cols = []\n",
        "\n",
        "#train_df, train_ohe_col0 = ohe(train_df, nominal_cols[0])\n",
        "#train_ohe_cols += train_ohe_col0\n",
        "\n",
        "#train_df, train_ohe_col1 = ohe(train_df, nominal_cols[1])\n",
        "#train_ohe_cols += train_ohe_col1\n",
        "\n",
        "#train_df, train_ohe_col2 = ohe(train_df, nominal_cols[2])\n",
        "#train_ohe_cols += train_ohe_col2\n",
        "\n",
        "#binary_cols += train_ohe_cols\n",
        "\n",
        "#train_df = train_df.cache()\n",
        "#print(train_df.count())\n",
        "#print(time() - t0)\n",
        "\n",
        "#t0 = time()\n",
        "#test_ohe_cols = []\n",
        "\n",
        "#test_df, test_ohe_col0_names = ohe(test_df, nominal_cols[0])\n",
        "#test_ohe_cols += test_ohe_col0_names\n",
        "\n",
        "#test_df, test_ohe_col1_names = ohe(test_df, nominal_cols[1])\n",
        "#test_ohe_cols += test_ohe_col1_names\n",
        "\n",
        "#test_df, test_ohe_col2_names = ohe(test_df, nominal_cols[2])\n",
        "#test_ohe_cols += test_ohe_col2_names\n",
        "\n",
        "#test_binary_cols = col_names[binary_inx].tolist() + test_ohe_cols\n",
        "\n",
        "#test_df = test_df.cache()\n",
        "#print(test_df.count())\n",
        "#print(time() - t0)\n",
        "\n",
        "def getAttributeRatio(df, numericCols, binaryCols, labelCol):\n",
        "    ratio_dict = {}\n",
        "    \n",
        "    if numericCols:\n",
        "        avg_dict = (df\n",
        "                .select(list(map(lambda c: sql.avg(c).alias(c), numericCols)))\n",
        "                .first()\n",
        "                .asDict())\n",
        "\n",
        "        ratio_dict.update(df\n",
        "                .groupBy(labelCol)\n",
        "                .avg(*numericCols)\n",
        "                .select(list(map(lambda c: sql.max(col('avg(' + c + ')')/avg_dict[c]).alias(c), numericCols)))\n",
        "                .fillna(0.0)\n",
        "                .first()\n",
        "                .asDict())\n",
        "    \n",
        "    if binaryCols:\n",
        "        ratio_dict.update((df\n",
        "                .groupBy(labelCol)\n",
        "                .agg(*list(map(lambda c: (sql.sum(col(c))/(sql.count(col(c)) - sql.sum(col(c)))).alias(c), binaryCols)))\n",
        "                .fillna(1000.0)\n",
        "                .select(*list(map(lambda c: sql.max(col(c)).alias(c), binaryCols)))\n",
        "                .first()\n",
        "                .asDict()))\n",
        "        \n",
        "    return OrderedDict(sorted(ratio_dict.items(), key=lambda v: -v[1]))\n",
        "\n",
        "def selectFeaturesByAR(ar_dict, min_ar):\n",
        "    return [f for f in ar_dict.keys() if ar_dict[f] >= min_ar]\n",
        "\n",
        "#t0 = time()\n",
        "#ar_dict = getAttributeRatio(train_df, numeric_cols, binary_cols, 'labels5')\n",
        "\n",
        "#print(len(ar_dict))\n",
        "#print(time() - t0)\n",
        "#ar_dict\n",
        "\n",
        "#t0 = time()\n",
        "#avg_dict = (train_df.select(list(map(lambda c: sql.avg(c).alias(c), numeric_cols))).first().asDict())\n",
        "#std_dict = (train_df.select(list(map(lambda c: sql.stddev(c).alias(c), numeric_cols))).first().asDict())\n",
        "\n",
        "def standardizer(column):\n",
        "    return ((col(column) - avg_dict[column])/std_dict[column]).alias(column)\n",
        "\n",
        "# Standardizer without mean\n",
        "# def standardizer(column):\n",
        "# return (col(column)/std_dict[column]).alias(column)\n",
        "\n",
        "train_scaler = [*binary_cols, *list(map(standardizer, numeric_cols)), *['id', 'labels2_index', 'labels2', 'labels5_index', 'labels5']]\n",
        "test_scaler = [*test_binary_cols, *list(map(standardizer, numeric_cols)), *['id', 'labels2_index', 'labels2', 'labels5_index', 'labels5']]\n",
        "\n",
        "scaled_train_df = (train_df.select(train_scaler).cache())\n",
        "scaled_test_df = (test_df.select(test_scaler).cache())\n",
        "\n",
        "#print(scaled_train_df.count())\n",
        "#print(scaled_test_df.count())\n",
        "#print(time() - t0)\n",
        "\n",
        "from pyspark.ml.feature import VectorIndexer, VectorAssembler\n",
        "assembler = VectorAssembler(inputCols=selectFeaturesByAR(ar_dict, 0.01), outputCol='raw_features')\n",
        "indexer = VectorIndexer(inputCol='raw_features', outputCol='indexed_features', maxCategories=2)\n",
        "\n",
        "prep_pipeline = Pipeline(stages=[assembler, indexer])\n",
        "prep_model = prep_pipeline.fit(scaled_train_df)\n",
        "\n",
        "#t0 = time()\n",
        "scaled_train_df = (prep_model\n",
        "        .transform(scaled_train_df)\n",
        "        .select('id', 'indexed_features', 'labels2_index', 'labels2', 'labels5_index', 'labels5')\n",
        "        .cache())\n",
        "\n",
        "scaled_test_df = (prep_model \n",
        "       .transform(scaled_test_df)\n",
        "       .select('id', 'indexed_features','labels2_index', 'labels2', 'labels5_index', 'labels5')\n",
        "       .cache())\n",
        "\n",
        "#print(scaled_train_df.count())\n",
        "#print(scaled_test_df.count())\n",
        "#print(time() - t0)\n",
        "\n",
        "seed = 4667979835606274383\n",
        "#print(seed)\n",
        "\n",
        "split = (scaled_train_df.randomSplit([0.8, 0.2], seed=seed))\n",
        "\n",
        "scaled_train_df = split[0].cache()\n",
        "scaled_cv_df = split[1].cache()\n",
        "\n",
        "#print(scaled_train_df.count())\n",
        "#print(scaled_cv_df.count())\n",
        "\n",
        "res_cv_df = scaled_cv_df.select(col('id'), col('labels2_index'), col('labels2'), col('labels5')).cache()\n",
        "res_test_df = scaled_test_df.select(col('id'), col('labels2_index'), col('labels2'), col('labels5')).cache()\n",
        "prob_cols = []\n",
        "pred_cols = []\n",
        "\n",
        "#print(res_cv_df.count())\n",
        "#print(res_test_df.count())\n",
        "\n",
        "def printCM(cm, labels):\n",
        "    \"\"\"pretty print for confusion matrixes\"\"\"\n",
        "    columnwidth = max([len(x) for x in labels])\n",
        "    # Print header\n",
        "    print(\" \" * columnwidth, end=\"\\t\")\n",
        "    for label in labels:\n",
        "        print(\"%{0}s\".format(columnwidth) % label, end=\"\\t\")\n",
        "    print()\n",
        "    # Print rows\n",
        "    for i, label1 in enumerate(labels):\n",
        "        print(\"%{0}s\".format(columnwidth) % label1, end=\"\\t\")\n",
        "        for j in range(len(labels)):\n",
        "            print(\"%{0}d\".format(columnwidth) % cm[i, j], end=\"\\t\")\n",
        "        print()\n",
        "\n",
        "def getPrediction(e):\n",
        "    return udf(lambda row: 1.0 if row >= e else 0.0, DoubleType())\n",
        "        \n",
        "def printReport(resDF, probCol, labelCol='labels2_index', e=None, labels=['normal', 'attack']):\n",
        "    if (e):\n",
        "        predictionAndLabels = list(zip(*resDF.rdd\n",
        "                                       .map(lambda row: (1.0 if row[probCol] >= e else 0.0, row[labelCol]))\n",
        "                                       .collect()))\n",
        "    else:\n",
        "        predictionAndLabels = list(zip(*resDF.rdd\n",
        "                                       .map(lambda row: (row[probCol], row[labelCol]))\n",
        "                                       .collect()))\n",
        "    \n",
        "    cm = metrics.confusion_matrix(predictionAndLabels[1], predictionAndLabels[0])\n",
        "    printCM(cm, labels)\n",
        "    print(\" \")\n",
        "    print(\"Accuracy = %g\" % (metrics.accuracy_score(predictionAndLabels[1], predictionAndLabels[0])))\n",
        "    print(\"AUC = %g\" % (metrics.roc_auc_score(predictionAndLabels[1], predictionAndLabels[0])))\n",
        "    print(\" \")\n",
        "    print(\"False Alarm Rate = %g\" % (cm[0][1]/(cm[0][0] + cm[0][1])))\n",
        "    print(\"Detection Rate = %g\" % (cm[1][1]/(cm[1][1] + cm[1][0])))\n",
        "    print(\"F1 score = %g\" % (metrics.f1_score(predictionAndLabels[1], predictionAndLabels[0], labels)))\n",
        "    print(\" \")\n",
        "    print(metrics.classification_report(predictionAndLabels[1], predictionAndLabels[0]))\n",
        "    print(\" \")\n",
        "\n",
        "t0 = time()\n",
        "pca_slicer = VectorSlicer(inputCol=\"indexed_features\", outputCol=\"features\", names=selectFeaturesByAR(ar_dict, 0.05))\n",
        "\n",
        "pca = PCA(k=2, inputCol=\"features\", outputCol=\"pca_features\")\n",
        "pca_pipeline = Pipeline(stages=[pca_slicer, pca])\n",
        "\n",
        "pca_train_df = pca_pipeline.fit(scaled_train_df).transform(scaled_train_df)\n",
        "print(time() - t0)\n",
        "\n",
        "t0 = time()\n",
        "viz_train_data = np.array(pca_train_df.rdd.map(lambda row: [*row['pca_features'], row['labels2_index'], row['labels5_index']]).collect())\n",
        "plt.figure()\n",
        "plt.scatter(x=viz_train_data[:,0], y=viz_train_data[:,1], c=viz_train_data[:,2], cmap=\"Set1\")\n",
        "plt.figure()\n",
        "plt.scatter(x=viz_train_data[:,0], y=viz_train_data[:,1], c=viz_train_data[:,3], cmap=\"Set1\")\n",
        "plt.show()\n",
        "print(time() - t0)\n",
        "\n",
        "kmeans_prob_col = 'kmeans_rf_prob'\n",
        "kmeans_pred_col = 'kmeans_rf_pred'\n",
        "\n",
        "prob_cols.append(kmeans_prob_col)\n",
        "pred_cols.append(kmeans_pred_col)\n",
        "\n",
        "# KMeans clustrering\n",
        "from pyspark.ml.clustering import KMeans\n",
        "\n",
        "t0 = time()\n",
        "kmeans_slicer = VectorSlicer(inputCol=\"indexed_features\", outputCol=\"features\", \n",
        "                             names=list(set(selectFeaturesByAR(ar_dict, 0.1)).intersection(numeric_cols)))\n",
        "\n",
        "kmeans = KMeans(k=8, initSteps=25, maxIter=100, featuresCol=\"features\", predictionCol=\"cluster\", seed=seed)\n",
        "\n",
        "kmeans_pipeline = Pipeline(stages=[kmeans_slicer, kmeans])\n",
        "\n",
        "kmeans_model = kmeans_pipeline.fit(scaled_train_df)\n",
        "\n",
        "kmeans_train_df = kmeans_model.transform(scaled_train_df).cache()\n",
        "kmeans_cv_df = kmeans_model.transform(scaled_cv_df).cache()\n",
        "kmeans_test_df = kmeans_model.transform(scaled_test_df).cache()\n",
        "\n",
        "print(time() - t0)\n",
        "\n",
        "# Function for describing the contents of the clusters\n",
        "def getClusterCrosstab(df, clusterCol='cluster'):\n",
        "    return (df.crosstab(clusterCol, 'labels2')\n",
        "              .withColumn('count', col('attack') + col('normal'))\n",
        "              .withColumn(clusterCol + '_labels2', col(clusterCol + '_labels2').cast('int'))\n",
        "              .sort(col(clusterCol +'_labels2').asc()))\n",
        "\n",
        "kmeans_crosstab = getClusterCrosstab(kmeans_train_df).cache()\n",
        "kmeans_crosstab.show(n=30)\n",
        "\n",
        "# Function for splitting clusters\n",
        "def splitClusters(crosstab):\n",
        "    exp = ((col('count') > 25) & (col('attack') > 0) & (col('normal') > 0))\n",
        "\n",
        "    cluster_rf = (crosstab\n",
        "        .filter(exp).rdd\n",
        "        .map(lambda row: (int(row['cluster_labels2']), [row['count'], row['attack']/row['count']]))\n",
        "        .collectAsMap())\n",
        "\n",
        "    cluster_mapping = (crosstab\n",
        "        .filter(~exp).rdd\n",
        "        .map(lambda row: (int(row['cluster_labels2']), 1.0 if (row['count'] <= 25) | (row['normal'] == 0) else 0.0))\n",
        "        .collectAsMap())\n",
        "    \n",
        "    return cluster_rf, cluster_mapping\n",
        "\n",
        "kmeans_cluster_rf, kmeans_cluster_mapping = splitClusters(kmeans_crosstab)\n",
        "\n",
        "print(len(kmeans_cluster_rf), len(kmeans_cluster_mapping))\n",
        "print(kmeans_cluster_mapping)\n",
        "kmeans_cluster_rf\n",
        "\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "\n",
        "# This function returns Random Forest models for provided clusters\n",
        "def getClusterModels(df, cluster_rf):\n",
        "    cluster_models = {}\n",
        "\n",
        "    labels_col = 'labels2_cl_index'\n",
        "    labels2_indexer.setOutputCol(labels_col)\n",
        "\n",
        "    rf_slicer = VectorSlicer(inputCol=\"indexed_features\", outputCol=\"rf_features\", \n",
        "                             names=selectFeaturesByAR(ar_dict, 0.05))\n",
        "\n",
        "    for cluster in cluster_rf.keys():\n",
        "        t1 = time()\n",
        "        rf_classifier = RandomForestClassifier(labelCol=labels_col, featuresCol='rf_features', seed=seed,\n",
        "                                               numTrees=500, maxDepth=20, featureSubsetStrategy=\"sqrt\")\n",
        "        \n",
        "        rf_pipeline = Pipeline(stages=[labels2_indexer, rf_slicer, rf_classifier])\n",
        "        cluster_models[cluster] = rf_pipeline.fit(df.filter(col('cluster') == cluster))\n",
        "        print(\"Finished %g cluster in %g ms\" % (cluster, time() - t1))\n",
        "        \n",
        "    return cluster_models\n",
        "\n",
        "# This utility function helps to get predictions/probabilities for the new data and return them into one dataframe\n",
        "def getProbabilities(df, probCol, cluster_mapping, cluster_models):\n",
        "    pred_df = (sqlContext.createDataFrame([], StructType([\n",
        "                    StructField('id', LongType(), False),\n",
        "                    StructField(probCol, DoubleType(), False)])))\n",
        "    \n",
        "    udf_map = udf(lambda cluster: cluster_mapping[cluster], DoubleType())\n",
        "    pred_df = pred_df.union(df.filter(col('cluster').isin(list(cluster_mapping.keys())))\n",
        "                            .withColumn(probCol, udf_map(col('cluster')))\n",
        "                            .select('id', probCol))\n",
        "\n",
        "                                       \n",
        "    for k in cluster_models.keys():\n",
        "        maj_label = cluster_models[k].stages[0].labels[0]\n",
        "        udf_remap_prob = udf(lambda row: float(row[0]) if (maj_label == 'attack') else float(row[1]), DoubleType())\n",
        "\n",
        "        pred_df = pred_df.union(cluster_models[k]\n",
        "                         .transform(df.filter(col('cluster') == k))\n",
        "                         .withColumn(probCol, udf_remap_prob(col('probability')))\n",
        "                         .select('id', probCol))\n",
        "\n",
        "    return pred_df\n",
        "\n",
        "# Training Random Forest classifiers for each of the clusters\n",
        "t0 = time()\n",
        "kmeans_cluster_models = getClusterModels(kmeans_train_df, kmeans_cluster_rf)\n",
        "print(time() - t0)\n",
        "\n"
      ]
    }
  ]
}