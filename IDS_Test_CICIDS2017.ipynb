{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IDS_Test_CICIDS2017.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOl3xZrQl4q+HBT39lrsd4b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jules-4LL4RT/Internship/blob/main/IDS_Test_CICIDS2017.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLfwhEpxVEQq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3cf75c1b-c9cd-42ca-fb64-f981d7ebf0b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/Jules-4LL4RT/Internship/main/newFridayAfternoonDDoSaa.csv\n",
            "18866176/18860544 [==============================] - 0s 0us/step\n",
            "18874368/18860544 [==============================] - 0s 0us/step\n",
            "/root/.keras/datasets/newFridayAfternoonDDoSaa.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(44999, 85)\n",
            "Read 44999 rows.\n",
            "\n",
            "22500 rows\n",
            "**Flow ID:17271 (76%)\n",
            "** Source IP:622 (2%)\n",
            "** Source Port:14056 (62%)\n",
            "** Destination IP:955 (4%)\n",
            "** Destination Port:3023 (13%)\n",
            "** Protocol:3 (0%)\n",
            "** Timestamp:30 (0%)\n",
            "** Flow Duration:18373 (81%)\n",
            "** Total Fwd Packets:152 (0%)\n",
            "** Total Backward Packets:162 (0%)\n",
            "**Total Length of Fwd Packets:1546 (6%)\n",
            "** Total Length of Bwd Packets:2015 (8%)\n",
            "** Fwd Packet Length Max:922 (4%)\n",
            "** Fwd Packet Length Min:98 (0%)\n",
            "** Fwd Packet Length Mean:1944 (8%)\n",
            "** Fwd Packet Length Std:2173 (9%)\n",
            "**Bwd Packet Length Max:830 (3%)\n",
            "** Bwd Packet Length Min:259 (1%)\n",
            "** Bwd Packet Length Mean:2218 (9%)\n",
            "** Bwd Packet Length Std:2232 (9%)\n",
            "** Flow Packets/s:18655 (82%)\n",
            "** Flow IAT Mean:18630 (82%)\n",
            "** Flow IAT Std:15677 (69%)\n",
            "** Flow IAT Max:14778 (65%)\n",
            "** Flow IAT Min:2587 (11%)\n",
            "**Fwd IAT Total:9187 (40%)\n",
            "** Fwd IAT Mean:11008 (48%)\n",
            "** Fwd IAT Std:9981 (44%)\n",
            "** Fwd IAT Max:9085 (40%)\n",
            "** Fwd IAT Min:2094 (9%)\n",
            "**Bwd IAT Total:9462 (42%)\n",
            "** Bwd IAT Mean:11147 (49%)\n",
            "** Bwd IAT Std:10407 (46%)\n",
            "** Bwd IAT Max:9415 (41%)\n",
            "** Bwd IAT Min:1114 (4%)\n",
            "**Fwd PSH Flags:2 (0%)\n",
            "** Bwd PSH Flags:1 (0%)\n",
            "** Fwd URG Flags:1 (0%)\n",
            "** Bwd URG Flags:1 (0%)\n",
            "** Fwd Header Length:359 (1%)\n",
            "** Bwd Header Length:378 (1%)\n",
            "**Fwd Packets/s:18580 (82%)\n",
            "** Bwd Packets/s:15058 (66%)\n",
            "** Min Packet Length:93 (0%)\n",
            "** Max Packet Length:960 (4%)\n",
            "** Packet Length Mean:3232 (14%)\n",
            "** Packet Length Std:3336 (14%)\n",
            "** Packet Length Variance:3335 (14%)\n",
            "**FIN Flag Count:2 (0%)\n",
            "** SYN Flag Count:2 (0%)\n",
            "** RST Flag Count:2 (0%)\n",
            "** PSH Flag Count:2 (0%)\n",
            "** ACK Flag Count:2 (0%)\n",
            "** URG Flag Count:2 (0%)\n",
            "** CWE Flag Count:1 (0%)\n",
            "** ECE Flag Count:2 (0%)\n",
            "** Down/Up Ratio:8 (0%)\n",
            "** Average Packet Size:3048 (13%)\n",
            "** Avg Fwd Segment Size:1944 (8%)\n",
            "** Avg Bwd Segment Size:2218 (9%)\n",
            "** Fwd Header Length.1:359 (1%)\n",
            "**Fwd Avg Bytes/Bulk:1 (0%)\n",
            "** Fwd Avg Packets/Bulk:1 (0%)\n",
            "** Fwd Avg Bulk Rate:1 (0%)\n",
            "** Bwd Avg Bytes/Bulk:1 (0%)\n",
            "** Bwd Avg Packets/Bulk:1 (0%)\n",
            "**Bwd Avg Bulk Rate:1 (0%)\n",
            "**Subflow Fwd Packets:152 (0%)\n",
            "** Subflow Fwd Bytes:1546 (6%)\n",
            "** Subflow Bwd Packets:162 (0%)\n",
            "** Subflow Bwd Bytes:2015 (8%)\n",
            "**Init_Win_bytes_forward:609 (2%)\n",
            "** Init_Win_bytes_backward:735 (3%)\n",
            "** act_data_pkt_fwd:107 (0%)\n",
            "** min_seg_size_forward:7 (0%)\n",
            "**Active Mean:4467 (19%)\n",
            "** Active Std:1022 (4%)\n",
            "** Active Max:4464 (19%)\n",
            "** Active Min:4392 (19%)\n",
            "**Idle Mean:2830 (12%)\n",
            "** Idle Std:1117 (4%)\n",
            "** Idle Max:2445 (10%)\n",
            "** Idle Min:3847 (17%)\n",
            "** Label:2 (0%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:96: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9bc6108a06ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m' Source IP'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' Destination IP'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Flow ID'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' Timestamp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mencode_text_dummy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m   \u001b[0;31m#elif name in [' Source IP',' Destination IP']:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;31m#name = ipaddress.ip_address(name)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-9bc6108a06ef>\u001b[0m in \u001b[0;36mencode_text_dummy\u001b[0;34m(df, name)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdummies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mdummy_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{name}-{x}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdummy_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdummies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3590\u001b[0m         \u001b[0;31m# see if we can slice the rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3591\u001b[0;31m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_index_sliceable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3593\u001b[0m             \u001b[0;31m# either we have a slice or we have a string that can be converted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36mconvert_to_index_sliceable\u001b[0;34m(obj, key)\u001b[0m\n\u001b[1;32m   2332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2333\u001b[0m         \u001b[0;31m# we are an actual column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2334\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2335\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m__contains__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4572\u001b[0m         \u001b[0mhash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4573\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4574\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4575\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOverflowError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4576\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow\n",
        "from tensorflow.keras.utils import get_file\n",
        "import os\n",
        "import io\n",
        "import re\n",
        "import csv\n",
        "from time import time\n",
        "from sklearn import metrics\n",
        "from scipy.stats import zscore\n",
        "from google.colab import files\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import ipaddress\n",
        "\n",
        "t0 = time()\n",
        "try:\n",
        "     path = get_file('newFridayAfternoonDDoSaa.csv', origin=\\\n",
        "     'https://raw.githubusercontent.com/Jules-4LL4RT/Internship/main/newFridayAfternoonDDoSaa.csv',archive_format=None)\n",
        "except:\n",
        "    print('Something went wrong, please try again')\n",
        "    raise\n",
        "\n",
        "#path = 'https://raw.githubusercontent.com/Jules-4LL4RT/Internship/main/newFridayMorning2aa.csv'\n",
        "\n",
        "print(path)\n",
        "df = pd.read_csv(path, sep=',',error_bad_lines=True)\n",
        "#df.to_csv(\"newFridayMorning3.csv\")\n",
        "#df = pd.read_csv(path)\n",
        "\n",
        "#Display amount of rows and columns\n",
        "print(df.shape)\n",
        "#Check we read all rows (lines) of the given dataset (Same as previous basically)\n",
        "print(\"Read {} rows.\".format(len(df)))\n",
        "\n",
        "#Sample X % of the dataset\n",
        "#Replace=false because same row can't be sampled 2 times\n",
        "df = df.sample(frac=0.50, replace=False) \n",
        "\n",
        "#Drop columns with missing values\n",
        "df.dropna(inplace=True,axis=1)\n",
        "\n",
        "excluded = ['  Fwd Header Length']\n",
        "df = df.drop(columns=excluded, errors='ignore')\n",
        "\n",
        "#Removing doubled columns\n",
        "#df.columns = df.columns.str.strip()\n",
        "#df = df.drop(columns=['Fwd Header Length.1'])\n",
        "#print(df.shape)\n",
        "\n",
        "#Display n columns and rows maximum\n",
        "#pd.set_option('display.max_columns', 6)\n",
        "#pd.set_option('display.max_rows', 6)\n",
        "#df[0:2]\n",
        "\n",
        " \n",
        "#Printing columns (titles, quantity...)\n",
        "\n",
        "def analyze(df):\n",
        "    print()\n",
        "    cols = df.columns.values\n",
        "    total = float(len(df))\n",
        "\n",
        "    print(\"{} rows\".format(int(total)))\n",
        "    for col in cols:\n",
        "        #Count amount of unique columbs\n",
        "        i=0\n",
        "        nbCol = df[col].unique()\n",
        "        unique_count = len(nbCol)\n",
        "        #Display columns titles\n",
        "        print(\"**{}:{} ({}%)\".format(col,unique_count,\\\n",
        "        int(((unique_count)/total)*100)))\n",
        "\n",
        "analyze(df)\n",
        "\n",
        "#SD stands for standard deviation\n",
        "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
        "    if mean is None:\n",
        "        mean = df[name].mean()\n",
        "\n",
        "    if sd is None:\n",
        "        sd = df[name].std()\n",
        "\n",
        "    df[name] = (df[name] - mean) / sd\n",
        "\n",
        "#Dummy variables (ex: temp can be hot, medium or cold)\n",
        "#Axis=1 means columns\n",
        "def encode_text_dummy(df, name):\n",
        "    dummies = pd.get_dummies(df[name])\n",
        "    for x in dummies.columns:\n",
        "        dummy_name = f\"{name}-{x}\"\n",
        "        df[dummy_name] = dummies[x]\n",
        "    df.drop(name, axis=1, inplace=True)\n",
        "\n",
        "#Encoding depends of data types\n",
        "for name in df.columns:\n",
        "  if name == ' Label':\n",
        "    pass\n",
        "  elif name in [' Source IP',' Destination IP', 'Flow ID',' Timestamp']:\n",
        "    encode_text_dummy(df,name)\n",
        "  #elif name in [' Source IP',' Destination IP']:\n",
        "    #name = ipaddress.ip_address(name)\n",
        "    #encode_numeric_zscore(df,name)\n",
        "  #elif name in ['Flow ID',' Timestamp']:\n",
        "    #encode_text_dummy(df,name)\n",
        "  else:\n",
        "    encode_numeric_zscore(df,name)\n",
        "\n",
        "#'Flow ID',' Source IP',' Destination IP',' Timestamp'  \n",
        "\n",
        "#print(df.columns.tolist())\n",
        "\n",
        "#Remove columns with missing values\n",
        "df.dropna(inplace=True,axis=1)\n",
        "\n",
        "#Classification\n",
        "x_columns = df.columns.drop(' Label')\n",
        "x = df[x_columns].values\n",
        "dummies = pd.get_dummies(df[' Label'])\n",
        "outcomes = dummies.columns\n",
        "num_classes = len(outcomes)\n",
        "y = dummies.values\n",
        "\n",
        "df.groupby(' Label')[' Label'].count()\n",
        "\n",
        "#print(num_classes)\n",
        "#print(df.dtypes)\n",
        "\n",
        "t1 = time()\n",
        "\n",
        "# Test/train split - X% test\n",
        "# Split into train/test\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    x, y, test_size=0.20, random_state=13)\n",
        "\n",
        "#Converting an input as an array (normally not needed)\n",
        "#x_train=np.asarray(x_train).astype(np.int_)\n",
        "#y_train=np.asarray(y_train).astype(np.int_)\n",
        "\n",
        "#Defining the model\n",
        "model = Sequential()\n",
        "#(Hidden) Layers\n",
        "#The first number is the node amount, input the number of variables\n",
        "#ReLU is for rectified linear unit activation function\n",
        "model.add(Dense(10, input_dim=x.shape[1], activation='relu'))\n",
        "model.add(Dense(50, input_dim=x.shape[1], activation='relu'))\n",
        "model.add(Dense(10, input_dim=x.shape[1], activation='relu'))\n",
        "model.add(Dense(1, kernel_initializer='normal'))\n",
        "model.add(Dense(y.shape[1],activation='softmax'))\n",
        "#loss function are big if prediction deviation is high\n",
        "#loss='categorical_crossentropy' for multiple output (good for classification tasks)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, \n",
        "                        patience=5, verbose=1, mode='auto',\n",
        "                           restore_best_weights=True)\n",
        "model.fit(x_train,y_train,validation_data=(x_test,y_test),\n",
        "          callbacks=[monitor],verbose=2,epochs=50)\n",
        "          #Verbose is graphical mode for epoch, 2 means one line per epoch\n",
        "          #Callbacks are to avoid overfitting, monitoring val_loss\n",
        "          #Patience is the amount of epochs after which we stop if no improvement\n",
        "\n",
        "#Accuracy\n",
        "prediction = model.predict(x_test)\n",
        "prediction = np.argmax(prediction,axis=1)\n",
        "y_eval = np.argmax(y_test,axis=1)\n",
        "\n",
        "score = metrics.accuracy_score(y_eval, prediction)\n",
        "print(\"Score: {}\".format(score))\n",
        "\n",
        "print(time() - t0)\n",
        "print(time() - t1)\n",
        "\n"
      ]
    }
  ]
}